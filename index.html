<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SXZZ1ZXKKD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-SXZZ1ZXKKD');
</script>

  <!-- <meta name=viewport content=“width=800”> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 11.5pt
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 11.5pt;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    heading2 {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 11.5pt;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    /* color of hyperlinks */
    a:link {
      color: #3B5998;
    }
    a:visited {
      color: #3B5998;
    }
    a:hover {
      color: #FF69B4;
    }
    a:active {
      color: #3B5998;
    }
    span.highlight {
        background-color: #ffffd0;
    }

    h1{text-align: right;}
  </style>
  <link rel="icon" type="image/png" href="./public/wave.png">
  <title>Shoukang Hu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
  <tr><td>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td width="67%" valign="middle">
  <p align="center">
  <name>Shoukang Hu</name>
  </p>
  <p style="text-align:justify">
  <font style="line-height:1.4;">I am a Senior Researcher at Microsoft Research Asia. 
    Before that, I was a Research Scientist at Sony AI and a Research Fellow at MMLab@NTU, working with <a href="https://liuziwei7.github.io">Prof. Ziwei Liu</a>. 
    I obtained the Ph.D. degree from The Chinese Univeristy of Hong Kong under the supervision of <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Prof. Xunying Liu</a>,
    and the B.E. Degree from the University of Electronic Science and Technology of China. 
  </font><br>
    <br>
    <font style="line-height:1.5;"><b>Email:</b> shoukang [dot] hu [at] gmail.com</font> 
  </p>
  <p align=center>
  <a href="https://scholar.google.com/citations?user=9cUPotAAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
  <a href="https://github.com/skhu101">GitHub</a> &nbsp|&nbsp
  <a href="https://drive.google.com/file/d/1JaQn3bVC4OzBbVooo0YbHVO4WRe-oAXC/view?usp=sharing">CV</a>
  </p>
  </font>
  <!-- <font style="line-height:1.5;"><b>Quote:</b> <i>"The two most powerful warriors are patience and time" - Leo Tolstoy</i></font> 
  </p> -->
  </td>
  <td width="33%">
  <img src="public/me.png"> <!-- should be 280x280 -->
  </td></tr></table>

  <!-- RESEARCH SECTION -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr> <td width="100%" valign="middle">
  <heading>Research Interests</heading> <p style="text-align:justify">
    <font style="line-height:1.4;">I am interested in exploring multiple modalities to enhance the advancement of perception, reconstruction, and generation, including 3D Human Modelling, Audio/Speech Processing and Automated Machine Learning, i.e.,</p>
    <ul><li><font style="line-height:1.5;"><strong>3D Human/Object Reconstruction/Generation</strong></font> <ul><li><font style="line-height:1.5;">[<a href="https://skhu101.github.io/HumanGif" target="_blank" rel="noopener noreferrer">HumanGif</a>] [<a href="https://skhu101.github.io/GauHuman" target="_blank" rel="noopener noreferrer">GauHuman</a>] [<a href="https://skhu101.github.io/HumanLiff" target="_blank" rel="noopener noreferrer">HumanLiff</a>] [<a href="https://skhu101.github.io/SHERF" target="_blank" rel="noopener noreferrer">SHERF</a>] [<a href="https://skhu101.github.io/ConsistentNeRF" target="_blank" rel="noopener noreferrer">ConsistentNeRF</a>]</font></li></ul> </font>
    <li><font style="line-height:1.5;"><strong>Automated Speech Recognition (ASR)</strong></font> <ul><li><font style="line-height:1.5;">[<a href="https://arxiv.org/abs/2012.04494" target="_blank" rel="noopener noreferrer">Bayesian LF-MMI ASR</a>] [<a href="https://arxiv.org/abs/2201.03943" target="_blank" rel="noopener noreferrer">NAS LF-MMI ASR</a>] [<a href="https://speechsystemdemo.github.io/demo/M14B2M3.html" target="_blank" rel="noopener noreferrer">CUHK Dysarthric ASR</a>]</font></li></ul> 
    <li><font style="line-height:1.5;"><strong>Automated Machine Learning (AutoML)</strong></font> <ul><li><font style="line-height:1.5;">[<a href="https://arxiv.org/abs/2203.15207" target="_blank" rel="noopener noreferrer">GM-NAS</a>][<a href="https://github.com/SNAS-Series/SNAS-Series" target="_blank" rel="noopener noreferrer">NAS-Analysis</a>][<a href="https://github.com/SNAS-Series/SNAS-Series" target="_blank" rel="noopener noreferrer">DSNAS</a>]</font></li></ul></li></ul>
    </p>
    <!-- <strong>Internship advice</strong>:
    I'm happy to collaborate with enthusiastic and talented PhD students. Currently, we have opportunities for research interns. If you are interested in working with me, e-mail me your CV and a brief description of your desired focus during the internship. -->

  </td> </tr> </table>

  <!-- NEWS SECTION -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr> <td width="100%" valign="middle">
    <heading>News</heading> <p>
      <ul>
        <li><font style="line-height:1.5;">[Jun. 2025] <a href="https://free4d.github.io/">Free4D (ICCV 2025)</a> (proj page, code, and demo) is released.</font></li>
        <li><font style="line-height:1.5;">[May. 2025] We are organizing <a href="https://avgen123.github.io/iccv25.html">2nd workshop on Audio-Visual Generation and Learning</a> at ICCV 2025.</font></li>
        <li><font style="line-height:1.5;">[May. 2025] <a href="https://skhu101.github.io/HumanLiff/">HumanLiff</a> is accepted by IJCV.</font></li>
        <li><font style="line-height:1.5;">[Feb. 2025] <a href="https://wildavatar.github.io/">WildAvatar (CVPR 2025)</a> (proj page, code, and demo) is released.</font></li>
        <li><font style="line-height:1.5;">[Feb. 2025] <a href="https://skhu101.github.io/HumanGif/">HumanGif</a> (proj page, code, and demo) is released.</font></li>
      </ul>
    </p> </td> </tr> 
  </table>

  <!-- Publications -->
  <table
    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px 20px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
          <p>(* indicates equal contribution, + denotes corresponding author)</p>
          <br>
        </td>
      </tr>
    </tbody>
  </table>

  <table
  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:top" align="center">
        <img src="./public/free4D_logo.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency
        </papertitle>
        <!-- authors -->
        <br>
        Tianqi Liu<sup>*</sup>, Zihao Huang<sup>*</sup>, Zhaoxi Chen, Guangcong Wang, <b>Shoukang Hu</b>, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li<sup>+</sup>, Ziwei Liu<sup>+</sup>
        <br>
        <!-- conference & date -->
        International Conference on Computer Vision (<strong>ICCV</strong>), 2025.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2503.20785">Paper</a>] [<a href="https://free4d.github.io/">Project Page</a>] [<a href="https://github.com/TQTQliu/Free4D">Code</a>] <img src="https://img.shields.io/github/stars/TQTQliu/Free4D?style=social"> <img src="https://img.shields.io/github/forks/TQTQliu/Free4D?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> Free4D is a tuning-free framework for 4D scene generation.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/HumanGif_thumbnail.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>HumanGif: Single-View Human Diffusion with Generative Prior
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://scholar.google.com/citations?user=D3h3NxwAAAAJ&hl=en" target="_blank">Takuya Narihira</a>,
        <a href="https://ai.sony/people/Kazumi-Fukuda/" target="_blank">Kazumi Fukuda</a>,
        <a href="https://ai.sony/people/Ryosuke-Sawata/" target="_blank">Ryosuke Sawata</a>,
        <a href="https://ai.sony/people/Takashi-Shibuya/" target="_blank">Takashi Shibuya</a>,
        <a href="https://www.yukimitsufuji.com/" target="_blank">Yuki Mitsufuji</a>
        <br>
        <!-- conference & date -->
        Preprint
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/pdf/2502.12080">Paper</a>] [<a href="https://skhu101.github.io/HumanGif/">Project Page</a>] [<a href="https://github.com/skhu101/HumanGif">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/HumanGif?style=social"> <img src="https://img.shields.io/github/forks/skhu101/HumanGif?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> HumanGif formulates the single-view-based 3D human novel view and pose synthesis as a <strong>single-view-conditioned human diffusion process</strong>, utilizing <strong>generative priors</strong> from foundational diffusion models.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:top" align="center">
        <img src="./public/wildavatar_logo.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>WildAvatar: Web-scale In-the-wild Video Dataset for 3D Avatar Creation
        </papertitle>
        <!-- authors -->
        <br>
        Zihao Huang, <b>Shoukang Hu</b>, Guangcong Wang, Tianqi Liu, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
        <br>
        <!-- conference & date -->
        Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2025.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/pdf/2407.02165v2">Paper</a>] [<a href="https://wildavatar.github.io/">Project Page</a>] [<a href="https://github.com/wildavatar/WildAvatar_Toolbox">Code</a>] <img src="https://img.shields.io/github/stars/wildavatar/WildAvatar_Toolbox?style=social"> <img src="https://img.shields.io/github/forks/wildavatar/WildAvatar_Toolbox?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> WildAvatar is a large-scale dataset from YouTube with 10,000+ human subjects, designed to address the limitations of existing laboratory datasets for avatar creation.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:top" align="center">
        <img src="./public/mvsgaussian_logo.gif" style="border-style: none"width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo
        </papertitle>
        <!-- authors -->
        <br>
        Tianqi Liu, Guangcong Wang, <b>Shoukang Hu</b>, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, Ziwei Liu.
        <br>
        <!-- conference & date -->
        European Conference on Computer Vision (<strong>ECCV</strong>), 2024
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2405.12218">Paper</a>] [<a href="https://mvsgaussian.github.io/">Project Page</a>] [<a href="https://github.com/TQTQliu/MVSGaussian">Code</a>] <img src="https://img.shields.io/github/stars/TQTQliu/MVSGaussian?style=social"> <img src="https://img.shields.io/github/forks/TQTQliu/MVSGaussian?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> MVSGaussian is a Gaussian-based method designed for efficient reconstruction of unseen scenes from sparse views in a single forward pass. It also offers high-quality initialization for fast training and real-time rendering.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/GauHuman_thumbnail.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        <br>
        <!-- conference & date -->
        Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2024.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/pdf/2312.02973.pdf">Paper</a>] [<a href="https://skhu101.github.io/GauHuman/">Project Page</a>] [<a href="https://github.com/skhu101/GauHuman">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/GauHuman?style=social"> <img src="https://img.shields.io/github/forks/skhu101/GauHuman?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> GauHuman learns articulated Gaussian Splatting from monocular videos with both <strong>fast training</strong> (1~2 minutes) and <strong>real-time rendering</strong> (up to 189 FPS).</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/HumanLiff_thumbnail.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>HumanLiff: Layer-wise 3D Human Generation with Diffusion Model
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://hongfz16.github.io/">Fangzhou Hong</a>,
        <a href="https://www.cs.umd.edu/~taohu/">Tao Hu</a>,
        <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ">Liang Pan</a>,
        Weiye Xiao,
        Haiyi Mei,
        <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en">Lei Yang</a>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        <br>
        <!-- conference & date -->
        International Journal of Computer Vision (<strong>IJCV</strong>).
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2308.09712">Paper</a>] [<a href="https://skhu101.github.io/HumanLiff/">Project Page</a>] [<a href="https://github.com/skhu101/HumanLiff">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/HumanLiff?style=social"> <img src="https://img.shields.io/github/forks/skhu101/HumanLiff?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> HumanLiff learns the layer-wise 3D human generative model with a unified diffusion process.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/ConsistentNeRF_thumbnail.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>ConsistentNeRF: Enhancing Neural Radiance Fields with 3D Consistency for Sparse View Synthesis
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://www.cs.ox.ac.uk/people/kaichen.zhou/">Kaichen Zhou</a>,
        Kaiyu Li,
        <a href="https://yulonghui.github.io/">Longhui Yu</a>,
        <a href="https://scholar.google.com.sg/citations?user=2p7x6OUAAAAJ&hl=en">Lanqing Hong</a>,
        <a href="https://scholar.google.com/citations?user=mlA_3r0AAAAJ&hl=en">Tianyang Hu</a>,
        <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en">Zhenguo	Li</a>,
        <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee	Lee</a>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        <br>
        <!-- conference & date -->
        Preprint
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/pdf/2305.11031.pdf">Paper</a>] [<a href="https://skhu101.github.io/ConsistentNeRF/">Project Page</a>] [<a href="https://github.com/skhu101/ConsistentNeRF">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/ConsistentNeRF?style=social"> <img src="https://img.shields.io/github/forks/skhu101/ConsistentNeRF?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> ConsistentNeRF Enhances Neural Radiance Fields with 3D Consistency for Sparse View Synthesis.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/ICCV2023_SHERF.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>SHERF: Generalizable Human NeRF from a Single Image
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu*</strong>,
        <a href="https://hongfz16.github.io/">Fangzhou Hong*</a>,
        <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ">Liang Pan</a>,
        Haiyi Mei,
        <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en">Lei Yang</a>,
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
        <br>
        <!-- conference & date -->
        International Conference on Computer Vision (<strong>ICCV</strong>), 2023.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2303.12791">Paper</a>] [<a href="https://skhu101.github.io/SHERF/">Project Page</a>] [<a href="https://github.com/skhu101/SHERF">code</a>] <img src="https://img.shields.io/github/stars/skhu101/SHERF?style=social"> <img src="https://img.shields.io/github/forks/skhu101/SHERF?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> SHERF learns a <strong>Generalizable Human NeRF</strong> to animate 3D humans from a single image.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/ICLR2022_GM_NAS_thumbnail.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Generalizing Few-Shot NAS with Gradient Matching
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu*</strong>,
        <a href="https://ruocwang.github.io">Ruocheng Wang*</a>,
        <a href="https://scholar.google.com/citations?user=2p7x6OUAAAAJ&hl=en&oi=ao">Lanqing Hong</a>,
        <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en">Zhenguo Li</a>,
        <a href="http://web.cs.ucla.edu/~chohsieh/">Cho-Jui Hsieh</a>,
        <a href="https://sites.google.com/site/jshfeng/">Jiashi Feng</a>
        <br>
        <!-- conference & date -->
        International Conference on Learning Representations (<strong>ICLR</strong>), 2022.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2203.15207">Paper</a>] [<a href="https://github.com/skhu101/GM-NAS">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/491512114">Zhihu</a>] <img src="https://img.shields.io/github/stars/skhu101/GM-NAS?style=social"> <img src="https://img.shields.io/github/forks/skhu101/GM-NAS?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> GM-NAS formulates supernet partitioning as <strong>a graph clustering problem</strong> and utilizes <strong>gradient matching score</strong> as the splitting criterion. Notably, we achieve <strong>80.6%</strong> accuracy on ImageNet under 600 flops constraint.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/TASLP_NAS_bottleneck_dim_search_space.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Neural Architecture Search For LF-MMI Trained Time Delay Neural Networks
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://scholar.google.com.hk/citations?user=bGD7wa0AAAAJ&hl=zh-CN">Xurong Xie</a>,
        <a href="https://scholar.google.com/citations?user=tONFEQcAAAAJ&hl=en">Mingyu Cui*</a>,
        <a href="https://www.researchgate.net/profile/Jiajun-Deng-4">Jiajun Deng*</a>,
        <a href="https://scholar.google.com/citations?user=ndVYtaUAAAAJ&hl=zh-CN">Shansong Liu</a>,
        <a href="https://scholar.google.com/citations?user=fY1IJ4wAAAAJ&hl=en">Jianwei Yu</a>,
        <a href="https://scholar.google.com/citations?user=RS59rgIAAAAJ&hl=en">Mengzhe Geng</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/">Helen Meng</a>
        <br>
        <!-- conference & date -->
        International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2021.<br>
        IEEE/ACM Transactions on Audio, Speech, and Language Processing (<strong>TASLP</strong>).
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2201.03943">Paper</a>] [<a href="https://github.com/skhu101/TDNN-F_NAS">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/TDNN-F_NAS?style=social"> <img src="https://img.shields.io/github/forks/skhu101/TDNN-F_NAS?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> We achieve <strong>9.9%/11.1%</strong> WER on Hub5'00/Rt03 test sets of 300-Hour Switchboard Task with 10.8M parameters.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/AISTATS2021_cost_gradient.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Understanding the wiring evolution in differentiable neural architecture search
        </papertitle>
        <!-- authors -->
        <br>
        <a href="https://siruixie.com">Sirui Xie*</a>,
        <strong>Shoukang Hu*</strong>,
        <a href="https://scholar.google.com/citations?user=q4lnWaoAAAAJ&hl=en">Xinjiang Wang</a>,
        <a href="https://scholar.google.com/citations?user=4m061tYAAAAJ&hl=en">Chunxiao Liu</a>,
        <a href="https://shijianping.me">Jianping Shi</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="http://dahua.me">Dahua Lin</a>
        <br>
        <!-- conference & date -->
        International Conference on Artificial Intelligence and Statistics (<strong>AISTATS</strong>), 2021.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2009.01272">Paper</a>] [<a href="https://github.com/SNAS-Series/SNAS-Series">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/357037023">Zhihu</a>] <img src="https://img.shields.io/github/stars/SNAS-Series/SNAS-Series?style=social"> <img src="https://img.shields.io/github/forks/SNAS-Series/SNAS-Series?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> Our analysis focuses on three observed searching patterns of differentiable NAS: 1) they search by growing instead of pruning; 2) wider networks are more preferred than deeper ones; 3) no edges are selected in bi-level optimization.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/CVPR2020_DSNAS_backward_slides_dsnas.gif" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>DSNAS: Direct Neural Architecture Search without Parameter Retraining
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu*</strong>,
        <a href="https://siruixie.com">Sirui Xie*</a>,
        <a href="https://scholar.google.com/citations?user=qTMA5BQAAAAJ&hl=en">Hehui Zheng</a>,
        <a href="https://scholar.google.com/citations?user=4m061tYAAAAJ&hl=en">Chunxiao Liu</a>,
        <a href="https://shijianping.me">Jianping Shi</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="http://dahua.me">Dahua Lin</a>
        <br>
        <!-- conference & date -->
        Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2020.
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2002.09128">Paper</a>] [<a href="https://github.com/SNAS-Series/SNAS-Series">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/141884849">Zhihu</a>] <img src="https://img.shields.io/github/stars/SNAS-Series/SNAS-Series?style=social"> <img src="https://img.shields.io/github/forks/SNAS-Series/SNAS-Series?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50"> We propose a new problem definition for NAS, i.e., task-specific end-to-end NAS. Our DSNAS got a final <strong>122</strong> review score.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/TASLP_Bayes_tdnn.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition
        </papertitle>
        <!-- authors -->
        <br>
        <strong>Shoukang Hu</strong>,
        <a href="https://scholar.google.com.hk/citations?user=bGD7wa0AAAAJ&hl=zh-CN">Xurong Xie</a>,
        <a href="https://scholar.google.com.hk/citations?user=R0E0bKkAAAAJ&hl=en">Max W. Y. Lam</a>,
        <a href="https://scholar.google.com/citations?user=ndVYtaUAAAAJ&hl=zh-CN">Shansong Liu</a>,
        <a href="https://scholar.google.com/citations?user=fY1IJ4wAAAAJ&hl=en">Jianwei Yu</a>,
        <a href="https://www.researchgate.net/profile/Zi-Ye-12">Zi Ye</a>,
        <a href="https://scholar.google.com/citations?user=RS59rgIAAAAJ&hl=en">Mengzhe Geng</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/">Helen Meng</a>
        <br>
        <!-- conference & date -->
        International Speech Communication Association (<strong>INTERSPEECH</strong>), 2018.<br>
        International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2019.<br>
        International Speech Communication Association (<strong>INTERSPEECH</strong>), 2019. <a HREF="https://signalprocessingsociety.org/community-involvement/speech-and-language-processing/newsletter/creation-yajie-miao-memorial-student" style="color:red">ISCA Yajie Miao Memorial Grant Winner </a> <br>
        In IEEE/ACM Transactions on Audio, Speech, and Language Processing (<strong>TASLP</strong>).
        <br>
        <!-- links -->
        [<a href="https://arxiv.org/abs/2012.04494">Paper</a>] [<a href="https://github.com/skhu101/Bayesian_TDNN.git">Code</a>] <img src="https://img.shields.io/github/stars/skhu101/Bayesian_TDNN?style=social"> <img src="https://img.shields.io/github/forks/skhu101/Bayesian_TDNN?style=social">
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50">We improve generalization ability of LF-MMI ASR with <strong>10.4%/11.8%</strong> WER on Hub5'00/Rt03 test sets of 300-Hour Switchboard task.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/TASLP_CUHK Dysarthric Speech_manual_arch.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Recent Progress in the CUHK Dysarthric Speech Recognition System
        </papertitle>
        <!-- authors -->
        <br>
        <a href="https://scholar.google.com/citations?user=ndVYtaUAAAAJ&hl=zh-CN">Shansong Liu*</a>,
        <a href="https://scholar.google.com/citations?user=RS59rgIAAAAJ&hl=en">Mengzhe Geng*</a>,
        <strong>Shoukang Hu*</strong>,
        <a href="https://scholar.google.com.hk/citations?user=bGD7wa0AAAAJ&hl=zh-CN">Xurong Xie*</a>,
        <a href="https://scholar.google.com/citations?user=tONFEQcAAAAJ&hl=en">Mingyu Cui</a>,
        <a href="https://scholar.google.com/citations?user=fY1IJ4wAAAAJ&hl=en">Jianwei Yu</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/">Helen Meng</a>
        <br>
        <!-- conference & date -->
        In IEEE/ACM Transactions on Audio, Speech, and Language Processing (<strong>TASLP</strong>).
        <br>
        <!-- links -->
        [<a href="https://ieeexplore.ieee.org/document/9463679">Paper</a>] [<a href="https://speechsystemdemo.github.io/demo/M14B2M3.html">Demo</a>] [<a href="https://www.isca-archive.org/interspeech_2019/hu19c_interspeech.html">Demo Paper</a>]
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50">We report our recent progress in CUHK Dysarthric Speech Recognition System.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/INTERSPEECH2019_pitch.png" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>On the Use of Pitch Features for Disordered Speech Recognition
        </papertitle>
        <!-- authors -->
        <br>
        <a href="https://scholar.google.com/citations?user=ndVYtaUAAAAJ&hl=zh-CN">Shansong Liu*</a>,
        <strong>Shoukang Hu*</strong>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/">Helen Meng</a>
        <br>
        <!-- conference & date -->
        International Speech Communication Association (<strong>INTERSPEECH</strong>), 2019.
        <br>
        <!-- links -->
        [<a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2609.pdf">Paper</a>] [<a href="https://speechsystemdemo.github.io/demo/M14B2M3.html">Demo</a>] [<a href="https://www.isca-archive.org/interspeech_2019/hu19c_interspeech.html">Demo Paper</a>]
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50">We investigate the use of pitch features in Disordered Speech Recognition.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/ICASSP2019_BLHUC.jpg" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>BLHUC: Bayesian learning of hidden unit contributions for deep neural network speaker adaptation
        </papertitle>
        <!-- authors -->
        <br>
        <a href="https://scholar.google.com.hk/citations?user=bGD7wa0AAAAJ&hl=zh-CN">Xurong Xie</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="http://www.ee.cuhk.edu.hk/~tanlee/">Tan Lee</a>,
        <strong>Shoukang Hu</strong>,
        <a href="https://www.researchgate.net/profile/Lan-Wang-58">Lan Wang</a>
        <br>
        <!-- conference & date -->
        International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2019. <a href="https://www.2019.ieeeicassp.org/2019.ieeeicassp.org/program.html#awards" style="color:red">Best Student Paper Award </a>
        <br>
        <!-- links -->
        [<a href="https://ieeexplore.ieee.org/document/8682667">Paper</a>] [<a href="https://github.com/XIEXurong/kaldi_bayes_adapt">Code</a>] 
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50">BLHUC achieves <strong>9.7%/10.7%</strong> WER on Hub5'00/Rt03 test sets of 300-Hour Switchboard task.</font></i>
        <br><br><br>
      </td>
    </tr>

    <tr>
      <td style="padding:10px 20px;width:25%;vertical-align:middle" align="center">
        <img src="./public/INTERSPEECH 2019_AVSR_gating.jpg" style="border-style: none" width="225">
      </td>
      <td width="75%" valign="middle">
        <!-- heading -->
        <papertitle>Exploiting Visual Features Using Bayesian Gated Neural Networks for Disordered Speech Recognition
        </papertitle>
        <!-- authors -->
        <br>
        <a href="https://scholar.google.com/citations?user=ndVYtaUAAAAJ&hl=zh-CN">Shansong Liu</a>,
        <strong>Shoukang Hu</strong>,
        Yi Wang,
        <a href="https://scholar.google.com/citations?user=fY1IJ4wAAAAJ&hl=en">Jianwei Yu</a>,
        <a href="https://dblp.org/pid/139/5460.html">Rongfeng Su</a>,
        <a href="https://www1.se.cuhk.edu.hk/~xyliu/">Xunying Liu</a>,
        <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/">Helen Meng</a>
        <br>
        <!-- conference & date -->
        International Speech Communication Association (<strong>INTERSPEECH</strong>), 2019. <a style="color:red">Best Student Paper Award Nomination </a>
        <br>
        <!-- links -->
        [<a href="https://www.isca-speech.org/archive/interspeech_2019/liu19j_interspeech.html">Paper</a>] [<a href="https://speechsystemdemo.github.io/demo/M14B2M3.html">Demo</a>] [<a href="https://www.isca-archive.org/interspeech_2019/hu19c_interspeech.html">Demo Paper</a>]
        <br><br>
        <!-- project description -->
        <i><font color="#FF7F50">Bayesian Gated Neural Networks achieves <strong>25.7%</strong> WER on UASpeech corpus.</font></i>
        <br><br><br>
      </td>
    </tr>

  </tbody>
  </table>


  <!-- Services SECTION -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr> <td width="100%" valign="middle">
  <heading>Services</heading> <p>
    <ul>
      <li><font style="line-height:1.5;">Conference PC Member: ICASSP 22-24, INTERSPEECH 21-23, NeurIPS 22-23, ICML 22-23, AAAI 22-23, IJCAI 23, AISTATS 21, SIGGRAPH 23</font></li>
      <li><font style="line-height:1.5;">Journal Reviewer: TASLP, TPAMI, TOG, TVCG, JMLR, IJCV, TNNLS, Neural Networks </font></li>
    </ul>
  </p> </td> </tr> 
  </table>

  <!-- Selected Rewards SECTION -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr> <td width="100%" valign="middle">
    <heading>Rewards</heading> <p>
      <ul>
        <li><font style="line-height:1.5;"><b>CUHK Postgraduate Student Scholarship</b></font></li>
        <li><font style="line-height:1.5;"><b>National Scholarship</b> awarded by the Ministry of Education of China in 2015 & 2016</font></li>
      </ul>
    </p> </td> </tr> 
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr> <td> <br>
  <p align="right"> <font size="2">
  <a href="https://jonbarron.info/">Good artists copy.</a>
	</font> </p> </td> </tr> </table>
    </td>
    </tr>
  </table>
  <p align="center">
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=m&d=l0ikMMOAMQeGt-AX7UvLWbfa5Seb3qC7MS83aFnqxws&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
  </p>
  </td>
  </tr>
  </table>



</body>
</html>